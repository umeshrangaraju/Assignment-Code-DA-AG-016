{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?**\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a simple, non-parametric, instance-based, and supervised machine learning algorithm that can be used for both classification and regression tasks. It is considered a 'lazy learner' because it does not build a model during the training phase but rather memorizes the entire training dataset. All computations are performed only when a prediction is requested.\n",
        "\n",
        "How KNN Works:\n",
        "Choose k: The first step is to choose the number of neighbors (k) that will be used to make the prediction. This is a crucial hyperparameter.\n",
        "\n",
        "Calculate Distance: For a new, unclassified data point, the algorithm calculates the distance (e.g., Euclidean distance, Manhattan distance) between this new point and every point in the training dataset.\n",
        "\n",
        "Identify k Nearest Neighbors: It then identifies the 'k' data points from the training set that are closest (i.e., have the smallest distance) to the new data point.\n",
        "\n",
        "Prediction: Based on these 'k' nearest neighbors, the algorithm makes a prediction. The method of prediction differs for classification and regression.\n",
        "\n",
        "KNN in Classification Problems:\n",
        "\n",
        "In classification, KNN predicts the class of a new data point based on the majority class among its 'k' nearest neighbors. For example:\n",
        "\n",
        "If k = 5, and among the 5 nearest neighbors, 3 belong to Class A and 2 belong to Class B, the new data point will be classified as Class A.\n",
        "It's common to use an odd number for 'k' to avoid ties in binary classification.\n",
        "\n",
        "KNN in Regression Problems:\n",
        "\n",
        "In regression, KNN predicts a continuous value for a new data point by taking the average (or weighted average) of the target values of its 'k' nearest neighbors. For example:\n",
        "\n",
        "If k = 5, and the target values of the 5 nearest neighbors are 10, 12, 11, 13, and 14, the predicted value for the new data point would be the average of these values: (10 + 12 + 11 + 13 + 14) / 5 = 12.\n",
        "\n",
        "Weighted average can also be used, where closer neighbors contribute more to the average.\n",
        "\n",
        "Key Characteristics and Considerations:\n",
        "\n",
        "Simplicity: Easy to understand and implement.\n",
        "No Training Phase: No explicit training phase, making it fast to 'train' but potentially slow for predictions with large datasets.\n",
        "Sensitive to Feature Scaling: Because it relies on distance metrics, KNN is very sensitive to the scale of the input features. Features with larger ranges can disproportionately influence the distance calculations. Feature scaling (e.g., normalization or standardization) is often necessary.\n",
        "\n",
        "Curse of Dimensionality: Performance degrades significantly with a high number of features (dimensions) as it becomes difficult for data points to be 'close' to each other in high-dimensional spaces.\n",
        "Choice of k: The value of 'k' is critical. A small 'k' can make the model sensitive to noise, while a large 'k' can make it ignore local patterns and smooth out the decision boundaries too much.\n",
        "Distance Metric: The choice of distance metric (e.g., Euclidean, Manhattan) can also impact performance.\n",
        "\n",
        "In summary, KNN is a versatile algorithm that can be effective for certain types of datasets, especially when the decision boundary is irregular, but it requires careful tuning of hyperparameters and preprocessing of data.\n",
        "\n",
        "In classification, KNN assigns a new data point to the class that is most frequent among its 'k' nearest neighbors. For instance, if 3 out of 5 nearest neighbors belong to Class A and 2 to Class B, the new point is classified as Class A. It's often recommended to use an odd 'k' to avoid ties.\n",
        "\n",
        "In regression, KNN predicts a continuous value for a new data point by calculating the average (or weighted average) of the target values of its 'k' nearest neighbors. For example, if the target values of 5 neighbors are 10, 12, 11, 13, and 14, the predicted value would be their average, which is 12."
      ],
      "metadata": {
        "id": "nrXNk3uE-sGg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?**\n",
        "\n",
        "\n",
        "\n",
        "The 'Curse of Dimensionality' refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces (i.e., with many features or variables) that do not occur in low-dimensional settings. As the number of dimensions increases, the volume of the space increases exponentially, making the available data sparse. This sparsity has significant implications for algorithms like K-Nearest Neighbors (KNN).\n",
        "\n",
        "How the Curse of Dimensionality Affects KNN Performance:\n",
        "Increased Sparsity: As the number of dimensions grows, the data points become increasingly sparse. This means that any given data point is very far from all other data points. In such a high-dimensional space, the concept of 'nearest neighbors' becomes less meaningful, as distances between points tend to converge.\n",
        "\n",
        "Distance Metric Degradation: KNN relies heavily on distance metrics (e.g., Euclidean distance) to find the 'nearest' neighbors. In high-dimensional spaces, the distance between any two points tends to become almost equal, making it difficult to distinguish between true neighbors and non-neighbors. All points appear to be 'far away' from each other, and the relative difference between the distances to the nearest and farthest neighbors diminishes.\n",
        "\n",
        "Increased Computational Cost: Calculating distances between a new data point and all training data points in a high-dimensional space is computationally expensive. Each dimension adds to the complexity of the distance calculation, making the prediction phase slower.\n",
        "\n",
        "Overfitting Risk: With sparse data in high dimensions, the model might find 'neighbors' that are actually far away in the intrinsic (lower-dimensional) manifold of the data. This can lead to KNN picking up noise in the high-dimensional space and overfitting to the training data, resulting in poor generalization performance on new, unseen data.\n",
        "\n",
        "Requirement for More Data: To combat sparsity in high dimensions, an exponentially larger amount of training data is required to ensure that there are enough samples to adequately represent the space. Acquiring such vast amounts of data is often impractical.\n",
        "\n",
        "In essence, the curse of dimensionality makes KNN less effective and computationally inefficient in high-dimensional spaces because the notion of 'proximity' or 'similarity' that KNN depends on breaks down. To mitigate this, techniques like dimensionality reduction (e.g., PCA, t-SNE) are often employed before applying KNN to high-dimensional datasets."
      ],
      "metadata": {
        "id": "X-IuvMMq_tI0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?**\n",
        "\n",
        "\n",
        "Principal Component Analysis (PCA) is a popular dimensionality reduction technique used in machine learning and statistics. Its primary goal is to transform a large set of correlated variables into a smaller set of uncorrelated variables, called principal components, while retaining as much of the original variance as possible.\n",
        "\n",
        "How PCA Works:\n",
        "Standardization: PCA is affected by scale, so it's essential to standardize the range of independent variables. This ensures that features with larger values don't dominate the components.\n",
        "Covariance Matrix Computation: This matrix summarizes the relationships (covariances) between all pairs of variables.\n",
        "Eigenvalue and Eigenvector Decomposition: Eigenvectors represent the directions (principal components) of maximum variance, and their corresponding eigenvalues indicate the magnitude of variance along those directions.\n",
        "Selecting Principal Components: The principal components are ranked by their corresponding eigenvalues in descending order. You choose a subset of these components (e.g., those explaining a high percentage of variance) to form the new feature subspace.\n",
        "Projection: The original data is then projected onto this new, lower-dimensional feature subspace.\n",
        "How PCA is different from Feature Selection:\n",
        "Both PCA and feature selection aim to reduce the number of features in a dataset, but they achieve this in fundamentally different ways:\n",
        "\n",
        "Nature of Features:\n",
        "\n",
        "PCA (Feature Extraction): Creates new, synthetic features (principal components) that are linear combinations of the original features. These new features are abstract and may not have a direct physical interpretation.\n",
        "Feature Selection: Selects a subset of the original features, discarding others. The selected features retain their original meaning and interpretability.\n",
        "Information Retention:\n",
        "\n",
        "PCA: Tries to preserve as much of the total variance from the original dataset as possible in the reduced set of dimensions. It compresses the information.\n",
        "Feature Selection: Discards features that are deemed irrelevant, redundant, or less important, potentially losing information contained in those discarded features.\n",
        "Interpretability:\n",
        "\n",
        "PCA: The principal components are typically not easily interpretable in terms of the original domain because they are linear combinations of many original features.\n",
        "Feature Selection: The selected features are directly interpretable, as they are the original features.\n",
        "Correlation Handling:\n",
        "\n",
        "PCA: Specifically designed to handle multicollinearity by creating uncorrelated principal components.\n",
        "Feature Selection: Can help reduce multicollinearity if highly correlated features are removed, but it doesn't explicitly transform features to be uncorrelated.\n",
        "When to use which:\n",
        "Use PCA when: You want to reduce dimensionality while retaining as much variance as possible, you are not concerned about the interpretability of individual features, and you need to handle multicollinearity.\n",
        "Use Feature Selection when: You need to maintain the interpretability of your features, you suspect some features are truly irrelevant or redundant, or your dataset has a very high number of features where new feature creation might still be complex."
      ],
      "metadata": {
        "id": "TKCL-eqZ_4_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?**\n",
        "\n",
        "Principal Component Analysis (PCA), eigenvalues and eigenvectors are fundamental mathematical concepts that are crucial for understanding how PCA works to reduce dimensionality. They are derived from the covariance matrix of the data.\n",
        "\n",
        "Eigenvectors:\n",
        "\n",
        "Definition: Eigenvectors are special vectors that, when a linear transformation (like multiplying by a matrix) is applied to them, only change by a scalar factor. They don't change their direction, only their magnitude. In PCA, eigenvectors represent the principal components.\n",
        "\n",
        "Role in PCA: Each eigenvector corresponds to a principal component. These eigenvectors define the new axes or directions in the feature space. They are orthogonal (perpendicular) to each other, meaning they capture independent dimensions of variance in the data. The first principal component (first eigenvector) points in the direction of the greatest variance in the data, the second in the direction of the second greatest variance orthogonal to the first, and so on.\n",
        "\n",
        "Eigenvalues:\n",
        "\n",
        "Definition: Eigenvalues are the scalar factors by which the eigenvectors are scaled during the linear transformation. Each eigenvector has a corresponding eigenvalue.\n",
        "\n",
        "Role in PCA: An eigenvalue quantifies the amount of variance captured along its corresponding eigenvector (principal component). A larger eigenvalue means that its corresponding eigenvector captures more variance from the original data. In simpler terms, the eigenvalue tells us how 'important' or 'significant' its corresponding principal component is in explaining the data's variability.\n",
        "\n",
        "Why are they important in PCA?\n",
        "\n",
        "Dimensionality Reduction: By calculating the eigenvalues and eigenvectors of the covariance matrix, PCA can identify the directions (eigenvectors) that capture the most variance (indicated by eigenvalues). We can then select a subset of these principal components (typically those with the largest eigenvalues) to form a new, lower-dimensional representation of the data, thereby reducing dimensionality while retaining as much information (variance) as possible.\n",
        "\n",
        "Variance Explanation: Eigenvalues allow us to determine the proportion of total variance explained by each principal component. This is often visualized using a 'scree plot', which helps in deciding how many principal components to retain.\n",
        "\n",
        "Data Transformation: The original data is projected onto the new coordinate system defined by the principal components (eigenvectors). This transforms the data into a new feature space where the new features (principal components) are uncorrelated.\n",
        "\n",
        "Identifying Patterns: By examining the eigenvectors, one can sometimes infer which original features contribute most to each principal component, offering insights into the underlying structure of the data.\n",
        "\n",
        "In summary, eigenvectors provide the directions of maximum variance (principal components), and eigenvalues indicate the magnitude of that variance along those directions. Together, they are the mathematical backbone of PCA, enabling it to effectively reduce the dimensionality of data while preserving its most significant features."
      ],
      "metadata": {
        "id": "pV6zNLolAT2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 5: How do KNN and PCA complement each other when applied in a single pipeline?**\n",
        "\n",
        "\n",
        "K-Nearest Neighbors (KNN) and Principal Component Analysis (PCA) complement each other effectively in a single machine learning pipeline, particularly when dealing with high-dimensional datasets. Their combined use leverages the strengths of each algorithm to mitigate their individual weaknesses. Here's how they complement each other:\n",
        "\n",
        "PCA as a Preprocessing Step for KNN:\n",
        "\n",
        "Addressing the Curse of Dimensionality: KNN's performance degrades significantly in high-dimensional spaces due to the 'curse of dimensionality'. PCA helps by reducing the number of features (dimensions) while retaining most of the data's variance. By projecting the data into a lower-dimensional subspace, PCA ensures that the concept of 'proximity' or 'nearest neighbors' remains meaningful for KNN.\n",
        "\n",
        "Improving Computational Efficiency: KNN is computationally expensive, especially during the prediction phase, as it needs to calculate distances to all training points for each new query point. When data is preprocessed by PCA, the number of dimensions is reduced, leading to faster distance calculations and overall improved training and prediction times for KNN.\n",
        "\n",
        "Reducing Noise and Redundancy: High-dimensional data often contains noisy and redundant features. PCA can effectively identify and eliminate these, as it focuses on directions of maximum variance. By removing noise and redundancy, PCA helps KNN to focus on the most informative features, potentially leading to better generalization and accuracy.\n",
        "\n",
        "Mitigating Feature Scaling Sensitivity: While PCA itself is sensitive to feature scaling, applying it after proper scaling can transform the original features into uncorrelated principal components. This step inherently handles the varying scales of original features that would otherwise disproportionately influence KNN's distance calculations.\n",
        "\n",
        "Preventing Overfitting: In high dimensions with sparse data, KNN can easily overfit by picking up noise as relevant patterns. PCA helps to reduce this risk by providing a more compact and less noisy representation of the data, which can lead to a more robust KNN model.\n",
        "\n",
        "How the Pipeline Works:\n",
        "\n",
        "Data Preprocessing (e.g., Scaling): Before applying PCA, it's often crucial to standardize or normalize the features, as PCA is sensitive to the scale of the data.\n",
        "\n",
        "PCA (Dimensionality Reduction): Apply PCA to the scaled data to transform it into a new set of principal components. Select a subset of these components that capture a significant amount of the total variance (e.g., 95%).\n",
        "\n",
        "KNN (Classification/Regression): Train the KNN model using the reduced-dimensional data (the selected principal components). When a new data point arrives, it is first transformed using the same PCA mapping and then fed to the trained KNN model for prediction.\n",
        "\n",
        "In essence, PCA acts as a powerful feature engineering and noise reduction technique that prepares high-dimensional data for KNN, making the latter more efficient, less prone to the curse of dimensionality, and potentially more accurate."
      ],
      "metadata": {
        "id": "Fu76B814BFfe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "541bb6a5",
        "outputId": "ccf4a868-c02c-42e4-973d-71841bde9b04"
      },
      "source": [
        "from sklearn.datasets import load_wine\n",
        "\n",
        "wine = load_wine()\n",
        "display(wine.keys())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names'])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset:\n",
        "Use the Wine Dataset from sklearn.datasets.load_wine().\n",
        "\n",
        "# **Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.**"
      ],
      "metadata": {
        "id": "TfD6TRiACBip"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9329b8d"
      },
      "source": [
        "# Task\n",
        "Analyze the impact of feature scaling on the K-Nearest Neighbors (KNN) classification model's accuracy using the Wine dataset. This involves preparing the data, training KNN models both with and without feature scaling, comparing their accuracies, and summarizing the findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "348d1642"
      },
      "source": [
        "## Prepare Data\n",
        "\n",
        "### Subtask:\n",
        "Extract features (X) and target (y) from the loaded Wine dataset. Then, split the data into training and testing sets to evaluate model performance fairly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ba6a1a8"
      },
      "source": [
        "**Reasoning**:\n",
        "I will extract the features (X) and target (y) from the loaded wine dataset and then split them into training and testing sets using `train_test_split` for model evaluation. This follows the instructions provided and prepares the data for subsequent steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90a5da92",
        "outputId": "f52f94a2-35f6-40c3-b40a-d6e6b79596a9"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Extract features (X) from the wine object\n",
        "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
        "\n",
        "# 2. Extract the target variable (y) from the wine object\n",
        "y = pd.Series(wine.target, name='target')\n",
        "\n",
        "# 3. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(\"Data preparation complete:\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preparation complete:\n",
            "X_train shape: (124, 13)\n",
            "X_test shape: (54, 13)\n",
            "y_train shape: (124,)\n",
            "y_test shape: (54,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cf78cd2"
      },
      "source": [
        "## Train KNN Model (Without Feature Scaling)\n",
        "\n",
        "### Subtask:\n",
        "Train a K-Nearest Neighbors (KNN) classifier on the original, unscaled training data (`X_train`, `y_train`). Then, evaluate its accuracy on the unscaled test data (`X_test`, `y_test`).\n",
        "\n",
        "#### Instructions:\n",
        "1. Import the `KNeighborsClassifier` from `sklearn.neighbors`.\n",
        "2. Instantiate a `KNeighborsClassifier` with `n_neighbors=5`.\n",
        "3. Fit the KNN model to the training data (`X_train`, `y_train`).\n",
        "4. Make predictions on the test data (`X_test`).\n",
        "5. Calculate the accuracy score of the model using `accuracy_score` from `sklearn.metrics` and print it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72688b63"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the data is prepared, the next step is to train a K-Nearest Neighbors (KNN) classifier using the original, unscaled training data and evaluate its performance on the unscaled test set, as outlined in the subtask. This will provide the baseline accuracy for comparison later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4650b2f2",
        "outputId": "ddc59885-5e62-4343-f7eb-2669f440f187"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Instantiate a KNeighborsClassifier with n_neighbors=5\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# 2. Fit the KNN model to the training data\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "\n",
        "# 3. Make predictions on the test data\n",
        "y_pred_unscaled = knn_unscaled.predict(X_test)\n",
        "\n",
        "# 4. Calculate the accuracy score\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "print(f\"Accuracy of KNN without feature scaling: {accuracy_unscaled:.4f}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of KNN without feature scaling: 0.7407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76dbde1c"
      },
      "source": [
        "## Apply Feature Scaling\n",
        "\n",
        "### Subtask:\n",
        "Scale the features of the training and testing datasets using `StandardScaler`.\n",
        "\n",
        "#### Instructions:\n",
        "1. Import `StandardScaler` from `sklearn.preprocessing`.\n",
        "2. Instantiate `StandardScaler`.\n",
        "3. Fit the scaler on the training data (`X_train`).\n",
        "4. Transform both the training (`X_train`) and testing (`X_test`) data using the fitted scaler. Store the results in `X_train_scaled` and `X_test_scaled` respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3b64cb2"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the previous subtask's introduction, I will now implement the feature scaling using `StandardScaler` on the training and testing datasets, as specified in the instructions. This will create the scaled data (`X_train_scaled`, `X_test_scaled`) necessary for training the KNN model with scaling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95cbddd1",
        "outputId": "9e38b20b-f3c4-4602-c254-0a0402d535cd"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Instantiate StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# 2. Fit the scaler on the training data (X_train)\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# 3. Transform both the training (X_train) and testing (X_test) data\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Feature scaling complete:\")\n",
        "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
        "print(f\"X_test_scaled shape: {X_test_scaled.shape}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature scaling complete:\n",
            "X_train_scaled shape: (124, 13)\n",
            "X_test_scaled shape: (54, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f3907f8"
      },
      "source": [
        "## Train KNN Model (With Feature Scaling)\n",
        "\n",
        "### Subtask:\n",
        "Train a K-Nearest Neighbors (KNN) classifier on the scaled training data (`X_train_scaled`, `y_train`). Then, evaluate its accuracy on the scaled test data (`X_test_scaled`, `y_test`).\n",
        "\n",
        "#### Instructions:\n",
        "1. Instantiate a new `KNeighborsClassifier` with `n_neighbors=5`.\n",
        "2. Fit this new KNN model to the scaled training data (`X_train_scaled`, `y_train`).\n",
        "3. Make predictions on the scaled test data (`X_test_scaled`).\n",
        "4. Calculate the accuracy score of the model using `accuracy_score` from `sklearn.metrics` and print it. Store this accuracy in a variable named `accuracy_scaled`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76c80d25"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the previous subtask's introduction, I will now train a KNN classifier using the scaled training data and evaluate its performance on the scaled test data. This will provide the accuracy of the KNN model with feature scaling, which can then be compared to the unscaled model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b888ee1",
        "outputId": "82fb35fb-9936-4109-a9d8-835e229e37a4"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Instantiate a new KNeighborsClassifier with n_neighbors=5\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# 2. Fit this new KNN model to the scaled training data\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 3. Make predictions on the scaled test data\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "\n",
        "# 4. Calculate the accuracy score\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy of KNN with feature scaling: {accuracy_scaled:.4f}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of KNN with feature scaling: 0.9630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22d5321f"
      },
      "source": [
        "## Compare Model Accuracy\n",
        "\n",
        "### Subtask:\n",
        "Compare the accuracy scores of the KNN model trained without feature scaling and the KNN model trained with feature scaling. Summarize the findings to understand the impact of feature scaling on KNN performance.\n",
        "\n",
        "#### Instructions:\n",
        "1. Print both `accuracy_unscaled` and `accuracy_scaled` for a direct comparison.\n",
        "2. Provide a brief explanation of why feature scaling significantly impacts KNN performance based on the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a88423e5"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous steps have calculated the accuracy for KNN both with and without feature scaling. Now, I need to print these accuracies for a direct comparison and provide an explanation of the impact of feature scaling, as per the current subtask's instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f05d250",
        "outputId": "bc7c0bbb-5005-43cf-ec7c-f32bdb7e4900"
      },
      "source": [
        "print(f\"Accuracy of KNN without feature scaling: {accuracy_unscaled:.4f}\")\n",
        "print(f\"Accuracy of KNN with feature scaling: {accuracy_scaled:.4f}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of KNN without feature scaling: 0.7407\n",
            "Accuracy of KNN with feature scaling: 0.9630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2429ff0"
      },
      "source": [
        "The results clearly demonstrate that feature scaling significantly improves the performance of the K-Nearest Neighbors (KNN) classifier on the Wine dataset.\n",
        "\n",
        "- **Accuracy of KNN without feature scaling**: 0.7407\n",
        "- **Accuracy of KNN with feature scaling**: 0.9630\n",
        "\n",
        "The accuracy increased from approximately 74% to 96% after applying `StandardScaler`. This substantial improvement is due to KNN being a distance-based algorithm. Without scaling, features with larger numerical ranges (e.g., 'proline' or 'magnesium' in the Wine dataset, which have values in hundreds) can disproportionately influence the distance calculations compared to features with smaller ranges (e.g., 'ash' or 'malic_acid', which have values around 1-5).\n",
        "\n",
        "When features are not scaled, the distance metric (like Euclidean distance) will be dominated by the features with larger values, effectively making the contributions of features with smaller ranges negligible. This distorts the true 'proximity' or 'similarity' between data points, leading to incorrect neighbor identification and thus poorer classification performance.\n",
        "\n",
        "By scaling the features, `StandardScaler` transforms the data such that each feature has a mean of 0 and a standard deviation of 1. This ensures that all features contribute equally to the distance calculations, allowing KNN to accurately identify the true nearest neighbors and make more reliable predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6730b78c"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the findings regarding the impact of feature scaling on KNN accuracy for the Wine dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "245f7812"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The impact of feature scaling on KNN accuracy for the Wine dataset is significant, leading to a substantial improvement in model performance.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   **Data Preparation:** The Wine dataset was successfully loaded and split into training and testing sets, with 124 samples for training and 54 samples for testing.\n",
        "*   **KNN without Feature Scaling:** A K-Nearest Neighbors (KNN) classifier, with \\$n\\_neighbors=5\\$, achieved an accuracy of 74.07% on the test set when trained on unscaled data.\n",
        "*   **Feature Scaling:** Standard scaling was applied to the features, transforming both the training and testing data to have a mean of 0 and a standard deviation of 1.\n",
        "*   **KNN with Feature Scaling:** After applying `StandardScaler`, a KNN classifier (with \\$n\\_neighbors=5\\$) achieved a significantly higher accuracy of 96.30% on the scaled test set.\n",
        "*   **Impact of Scaling:** Feature scaling increased the KNN model's accuracy by approximately 22.23 percentage points (from 74.07% to 96.30%), demonstrating its critical role for distance-based algorithms.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   Feature scaling is crucial for distance-based algorithms like KNN, as it prevents features with larger numerical ranges from disproportionately influencing distance calculations and ensures all features contribute equally to the model's predictions.\n",
        "*   For future analysis, explore the impact of different feature scaling methods (e.g., Min-Max Scaling) or optimize the `n_neighbors` hyperparameter for both scaled and unscaled data to see if further improvements can be achieved.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.**\n",
        "\n",
        "\n",
        "PCA model using the scaled Wine dataset and then print the explained variance ratio for each principal component.\n",
        "\n",
        "The PCA model has successfully identified the principal components and calculated their explained variance ratios. Here's what the output means:\n",
        "\n",
        "Explained Variance Ratio of Each Principal Component:\n",
        "This shows the proportion of the dataset's total variance that each individual principal component captures:\n",
        "\n",
        "Principal Component 1: Captures 0.3620 (36.20%) of the total variance.\n",
        "Principal Component 2: Captures 0.1876 (18.76%) of the total variance.\n",
        "...and so on.\n",
        "As expected, the first principal component explains the most variance, and each subsequent component explains less variance than the one before it.\n",
        "\n",
        "Cumulative Explained Variance:\n",
        "This shows the total proportion of variance captured by considering principal components up to a certain point:\n",
        "\n",
        "Up to Principal Component 1: Accounts for 0.3620 (36.20%) of the variance.\n",
        "Up to Principal Component 2: Accounts for 0.5496 (54.96%) of the variance (36.20% + 18.76%).\n",
        "Up to Principal Component 3: Accounts for 0.6662 (66.62%) of the variance.\n",
        "Up to Principal Component 13: Accounts for 1.0000 (100%) of the variance, as all original dimensions are considered.\n",
        "Key Insight:\n",
        "The cumulative explained variance is crucial for dimensionality reduction. For instance, if you wanted to retain approximately 95% of the variance, you would only need to keep the first 10 principal components (which capture 0.9616 or 96.16% of the variance). This means you could reduce the dimensionality from 13 original features down to 10 principal components without losing much information.\n",
        "\n",
        "This demonstrates how PCA effectively compresses the information from the original features into a smaller set of uncorrelated components."
      ],
      "metadata": {
        "id": "ihsz4TraCrh9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2397d12",
        "outputId": "1ea6e08c-7cdf-4336-c4bc-b497d9ca083b"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Instantiate PCA. We can set n_components to None to keep all components\n",
        "# and then inspect their explained variance ratio.\n",
        "pca = PCA(n_components=None)\n",
        "\n",
        "# Fit PCA on the scaled training data\n",
        "pca.fit(X_train_scaled)\n",
        "\n",
        "# Print the explained variance ratio of each principal component\n",
        "print(\"Explained variance ratio of each principal component:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"Principal Component {i+1}: {ratio:.4f}\")\n",
        "\n",
        "# Optionally, print the cumulative explained variance\n",
        "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
        "print(\"\\nCumulative explained variance:\")\n",
        "for i, cum_ratio in enumerate(cumulative_variance):\n",
        "    print(f\"Up to Principal Component {i+1}: {cum_ratio:.4f}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio of each principal component:\n",
            "Principal Component 1: 0.3620\n",
            "Principal Component 2: 0.1876\n",
            "Principal Component 3: 0.1166\n",
            "Principal Component 4: 0.0758\n",
            "Principal Component 5: 0.0704\n",
            "Principal Component 6: 0.0455\n",
            "Principal Component 7: 0.0358\n",
            "Principal Component 8: 0.0265\n",
            "Principal Component 9: 0.0217\n",
            "Principal Component 10: 0.0196\n",
            "Principal Component 11: 0.0176\n",
            "Principal Component 12: 0.0132\n",
            "Principal Component 13: 0.0076\n",
            "\n",
            "Cumulative explained variance:\n",
            "Up to Principal Component 1: 0.3620\n",
            "Up to Principal Component 2: 0.5496\n",
            "Up to Principal Component 3: 0.6662\n",
            "Up to Principal Component 4: 0.7420\n",
            "Up to Principal Component 5: 0.8124\n",
            "Up to Principal Component 6: 0.8579\n",
            "Up to Principal Component 7: 0.8938\n",
            "Up to Principal Component 8: 0.9202\n",
            "Up to Principal Component 9: 0.9420\n",
            "Up to Principal Component 10: 0.9616\n",
            "Up to Principal Component 11: 0.9792\n",
            "Up to Principal Component 12: 0.9924\n",
            "Up to Principal Component 13: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.**\n",
        "\n",
        "create a scree plot to visualize the explained variance ratio for each principal component. This plot will help in understanding how much variance each component captures and how many components might be sufficient to retain a significant portion of the total variance.\n",
        "\n",
        "The scree plot has been successfully generated and displayed. This plot visually represents the explained variance ratio for each principal component and the cumulative explained variance.\n",
        "\n",
        "The blue bars show the individual explained variance ratio for each principal component. You can see that the first few components (especially PC1 and PC2) explain a significantly larger proportion of the variance compared to the later components.\n",
        "\n",
        "The orange line represents the cumulative explained variance. This line helps you determine how many principal components are needed to capture a certain percentage of the total variance in your dataset. For example, you can observe that retaining the first 3-4 components already captures a substantial amount of the variance, and by around 10 components, almost all variance (over 95%) is accounted for.\n",
        "\n",
        "This plot is a crucial tool for deciding the optimal number of principal components to keep when performing dimensionality reduction with PCA."
      ],
      "metadata": {
        "id": "udysLv6CDB2c"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "60c95145",
        "outputId": "e015db3d-4f9d-4cba-9711-4a1edbaf80b9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get explained variance ratio and cumulative explained variance from the previously fitted PCA model\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
        "\n",
        "# Create a range for the number of components\n",
        "components = range(1, len(explained_variance_ratio) + 1)\n",
        "\n",
        "# Create the scree plot\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "# Plot individual explained variance ratio\n",
        "plt.bar(components, explained_variance_ratio, alpha=0.5, align='center', label='Individual Explained Variance')\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "plt.step(components, cumulative_variance, where='mid', label='Cumulative Explained Variance')\n",
        "\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.title('Scree Plot: Explained Variance by Principal Component')\n",
        "plt.xticks(components) # Ensure all component numbers are shown on x-axis\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explained variance ratio of each principal component:     \n",
        "Principal Component 1: 0.3620     \n",
        "Principal Component 2: 0.1876     \n",
        "Principal Component 3: 0.1166   \n",
        "Principal Component 4: 0.0758     \n",
        "Principal Component 5: 0.0704     \n",
        "Principal Component 6: 0.0455     \n",
        "Principal Component 7: 0.0358   \n",
        "Principal Component 8: 0.0265   \n",
        "Principal Component 9: 0.0217     \n",
        "Principal Component 10: 0.0196      \n",
        "Principal Component 11: 0.0176      \n",
        "Principal Component 12: 0.0132    \n",
        "Principal Component 13: 0.0076    \n",
        "\n",
        "Cumulative explained variance:      \n",
        "Up to Principal Component 1: 0.3620     \n",
        "Up to Principal Component 2: 0.5496   \n",
        "Up to Principal Component 3: 0.6662   \n",
        "Up to Principal Component 4: 0.7420   \n",
        "Up to Principal Component 5: 0.8124   \n",
        "Up to Principal Component 6: 0.8579     \n",
        "Up to Principal Component 7: 0.8938   \n",
        "Up to Principal Component 8: 0.9202     \n",
        "Up to Principal Component 9: 0.9420   \n",
        "Up to Principal Component 10: 0.9616    \n",
        "Up to Principal Component 11: 0.9792    \n",
        "Up to Principal Component 12: 0.9924    \n",
        "Up to Principal Component 13: 1.0000    "
      ],
      "metadata": {
        "id": "1AKVPlwbkBLk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abd197e6"
      },
      "source": [
        "# **Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.**\n",
        "\n",
        " KNN classifier on the PCA-transformed dataset using the top 2 components, and then compare its accuracy with the models trained on the original dataset. This will help us understand the impact of dimensionality reduction on KNN performance.\n",
        "\n",
        "Apply PCA with 2 Components: Transform the scaled training and testing data using PCA, retaining only the top 2 principal components. This will reduce the dimensionality of the dataset.\n",
        "\n",
        "Train KNN on PCA Data: Train a K-Nearest Neighbors (KNN) classifier on the 2-component PCA-transformed training data. Use the same n_neighbors=5 as before for consistency.\n",
        "\n",
        "Evaluate KNN on PCA Data: Make predictions using the KNN model trained on PCA data and the 2-component PCA-transformed test data. Calculate and print the accuracy.\n",
        "\n",
        "Compare All Accuracies: Compare the accuracy of the KNN model trained on PCA-transformed data with the accuracies of the KNN models trained on unscaled and scaled original data. Discuss the findings and the trade-offs involved with dimensionality reduction.\n",
        "\n",
        "Final Task: Summarize the findings regarding the impact of PCA on KNN accuracy and how it compares to using scaled and unscaled original features.\n",
        "\n",
        "# Task\n",
        "## Apply PCA with 2 Components\n",
        "\n",
        "### Subtask:\n",
        "Apply PCA to the scaled training and testing data, reducing the dimensionality to 2 principal components.\n",
        "\n",
        "#### Instructions:\n",
        "1. Instantiate `PCA` from `sklearn.decomposition` with `n_components=2`.\n",
        "2. Fit the PCA model on the scaled training data (`X_train_scaled`).\n",
        "3. Transform both the scaled training data (`X_train_scaled`) and scaled test data (`X_test_scaled`) using the fitted PCA model. Store the results in `X_train_pca` and `X_test_pca` respectively.\n",
        "\n",
        "### Reasoning:\n",
        "Given the previous analysis of explained variance, selecting 2 components for PCA will allow us to investigate the impact of significant dimensionality reduction on KNN's performance. By transforming both the training and testing data, we ensure consistency in the feature space for the subsequent KNN model. This aligns with the overall goal of evaluating KNN with PCA-transformed data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54fda650"
      },
      "source": [
        "## Apply PCA with 2 Components\n",
        "\n",
        "### Subtask:\n",
        "Transform the scaled training and testing data using PCA, retaining only the top 2 principal components. This will reduce the dimensionality of the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "477a8032"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires applying PCA with 2 components to the scaled training and testing data. I will instantiate PCA, fit it on the scaled training data, and then transform both the scaled training and testing data accordingly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "7abb5cb6",
        "outputId": "e1d2b5b2-b346-4ce5-8521-19acf0488350"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 1. Instantiate PCA with n_components=2\n",
        "pca_2_components = PCA(n_components=2)\n",
        "\n",
        "# 2. Fit the PCA model on the scaled training data\n",
        "pca_2_components.fit(X_train_scaled)\n",
        "\n",
        "# 3. Transform both the scaled training and testing data\n",
        "X_train_pca = pca_2_components.transform(X_train_scaled)\n",
        "X_test_pca = pca_2_components.transform(X_test_scaled)\n",
        "\n",
        "print(\"PCA transformation complete with 2 components:\")\n",
        "print(f\"X_train_pca shape: {X_train_pca.shape}\")\n",
        "print(f\"X_test_pca shape: {X_test_pca.shape}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train_scaled' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2700793011.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 2. Fit the PCA model on the scaled training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpca_2_components\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 3. Transform both the scaled training and testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train_scaled' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd9f0677"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed because `X_train_scaled` was not defined. This means the feature scaling step was not executed. To resolve this, I will combine the feature scaling step and the PCA application step into a single code block, ensuring that `X_train_scaled` and `X_test_scaled` are created before being used by PCA.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "1933b9f2",
        "outputId": "1be13e41-ab70-4a56-c0ab-df01a476d04a"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# --- Step 1: Apply Feature Scaling (re-running this to define X_train_scaled and X_test_scaled) ---\n",
        "# Instantiate StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data (X_train) and transform both training and testing data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Feature scaling complete and variables defined:\")\n",
        "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
        "print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n",
        "\n",
        "# --- Step 2: Apply PCA with 2 Components ---\n",
        "# 1. Instantiate PCA with n_components=2\n",
        "pca_2_components = PCA(n_components=2)\n",
        "\n",
        "# 2. Fit the PCA model on the scaled training data\n",
        "pca_2_components.fit(X_train_scaled)\n",
        "\n",
        "# 3. Transform both the scaled training and testing data\n",
        "X_train_pca = pca_2_components.transform(X_train_scaled)\n",
        "X_test_pca = pca_2_components.transform(X_test_scaled)\n",
        "\n",
        "print(\"\\nPCA transformation complete with 2 components:\")\n",
        "print(f\"X_train_pca shape: {X_train_pca.shape}\")\n",
        "print(f\"X_test_pca shape: {X_test_pca.shape}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2546990547.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Fit the scaler on the training data (X_train) and transform both training and testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mX_train_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mX_test_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4acb5bb2"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed because `X_train` and `X_test` were not defined, as the data preparation step was not re-executed. To fix this, I will combine the data preparation (extracting X and y, and splitting into train/test), feature scaling, and PCA application into a single code block, ensuring all necessary variables are defined sequentially.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "963da6dd",
        "outputId": "7aafd36e-0c61-4777-fdf5-e10a7830ad8a"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# --- Step 0: Re-run Data Preparation (to define X_train, X_test, y_train, y_test) ---\n",
        "# Assuming 'wine' object is already loaded from sklearn.datasets.load_wine() in a previous cell\n",
        "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
        "y = pd.Series(wine.target, name='target')\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(\"Data preparation re-executed:\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "\n",
        "# --- Step 1: Apply Feature Scaling ---\n",
        "# Instantiate StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data (X_train) and transform both training and testing data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"\\nFeature scaling complete and variables defined:\")\n",
        "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
        "print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n",
        "\n",
        "# --- Step 2: Apply PCA with 2 Components ---\n",
        "# 1. Instantiate PCA with n_components=2\n",
        "pca_2_components = PCA(n_components=2)\n",
        "\n",
        "# 2. Fit the PCA model on the scaled training data\n",
        "pca_2_components.fit(X_train_scaled)\n",
        "\n",
        "# 3. Transform both the scaled training and testing data\n",
        "X_train_pca = pca_2_components.transform(X_train_scaled)\n",
        "X_test_pca = pca_2_components.transform(X_test_scaled)\n",
        "\n",
        "print(\"\\nPCA transformation complete with 2 components:\")\n",
        "print(f\"X_train_pca shape: {X_train_pca.shape}\")\n",
        "print(f\"X_test_pca shape: {X_test_pca.shape}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'wine' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1882643875.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# --- Step 0: Re-run Data Preparation (to define X_train, X_test, y_train, y_test) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Assuming 'wine' object is already loaded from sklearn.datasets.load_wine() in a previous cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wine' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "966ab5c5"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed because the `wine` object was not defined. To resolve this, I need to ensure the `wine` dataset is loaded at the beginning of the code block before it is used. I will add `from sklearn.datasets import load_wine` and `wine = load_wine()` to the start of the code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "227b9e05",
        "outputId": "47984b16-88a3-4eb4-829d-9d9d56f152c2"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_wine # Import and load wine dataset\n",
        "\n",
        "# --- Step 0: Re-run Data Preparation (to define X_train, X_test, y_train, y_test) ---\n",
        "wine = load_wine()\n",
        "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
        "y = pd.Series(wine.target, name='target')\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(\"Data preparation re-executed:\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "\n",
        "# --- Step 1: Apply Feature Scaling ---\n",
        "# Instantiate StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data (X_train) and transform both training and testing data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"\\nFeature scaling complete and variables defined:\")\n",
        "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
        "print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n",
        "\n",
        "# --- Step 2: Apply PCA with 2 Components ---\n",
        "# 1. Instantiate PCA with n_components=2\n",
        "pca_2_components = PCA(n_components=2)\n",
        "\n",
        "# 2. Fit the PCA model on the scaled training data\n",
        "pca_2_components.fit(X_train_scaled)\n",
        "\n",
        "# 3. Transform both the scaled training and testing data\n",
        "X_train_pca = pca_2_components.transform(X_train_scaled)\n",
        "X_test_pca = pca_2_components.transform(X_test_scaled)\n",
        "\n",
        "print(\"\\nPCA transformation complete with 2 components:\")\n",
        "print(f\"X_train_pca shape: {X_train_pca.shape}\")\n",
        "print(f\"X_test_pca shape: {X_test_pca.shape}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preparation re-executed:\n",
            "X_train shape: (124, 13)\n",
            "X_test shape: (54, 13)\n",
            "\n",
            "Feature scaling complete and variables defined:\n",
            "X_train_scaled shape: (124, 13)\n",
            "X_test_scaled shape: (54, 13)\n",
            "\n",
            "PCA transformation complete with 2 components:\n",
            "X_train_pca shape: (124, 2)\n",
            "X_test_pca shape: (54, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05c0cbf4"
      },
      "source": [
        "## Train KNN on PCA Data\n",
        "\n",
        "### Subtask:\n",
        "Train a K-Nearest Neighbors (KNN) classifier on the 2-component PCA-transformed training data. Use the same n_neighbors=5 as before for consistency.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e48bd38"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the data has been scaled and transformed using PCA, the next step is to train a K-Nearest Neighbors (KNN) classifier on this reduced-dimensional data, as specified in the subtask instructions. This will allow for evaluation of KNN's performance with PCA.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e9332c2",
        "outputId": "d8b7c7d0-a35b-4fc7-b90f-1460419b687c"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# 1. Instantiate a KNeighborsClassifier with n_neighbors=5\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# 2. Fit the KNN model to the PCA-transformed training data\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "\n",
        "print(\"KNN model trained on PCA-transformed data.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN model trained on PCA-transformed data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d79b35e2"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the KNN model has been trained on the PCA-transformed data, the next logical step is to evaluate its performance. This involves making predictions on the PCA-transformed test data and then calculating the accuracy score, as per the overall task of comparing accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6f013e0",
        "outputId": "11ab1f6b-9de0-4d2f-edcd-7e0f6f3488ba"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Make predictions on the PCA-transformed test data\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "\n",
        "# Calculate the accuracy score\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "print(f\"Accuracy of KNN with 2-component PCA: {accuracy_pca:.4f}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of KNN with 2-component PCA: 0.9815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "999f2507"
      },
      "source": [
        "**Reasoning**:\n",
        "To compare the accuracy of KNN models across different data preparations, I need to print all relevant accuracy scores: unscaled, scaled, and PCA-transformed. This will facilitate a direct comparison as requested by the overall task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "9b906c28",
        "outputId": "59163dd4-daae-4f0f-aac3-398a97160125"
      },
      "source": [
        "print(f\"Accuracy of KNN without feature scaling: {accuracy_unscaled:.4f}\")\n",
        "print(f\"Accuracy of KNN with feature scaling: {accuracy_scaled:.4f}\")\n",
        "print(f\"Accuracy of KNN with 2-component PCA: {accuracy_pca:.4f}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'accuracy_unscaled' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3995041073.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Accuracy of KNN without feature scaling: {accuracy_unscaled:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Accuracy of KNN with feature scaling: {accuracy_scaled:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Accuracy of KNN with 2-component PCA: {accuracy_pca:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'accuracy_unscaled' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "957ccb22"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed because `accuracy_unscaled` and `accuracy_scaled` were not defined. This indicates that the preceding code cells where these accuracies were calculated were not executed. To resolve this, I will re-execute the necessary steps, including data preparation, unscaled KNN training, feature scaling, scaled KNN training, PCA application, and finally, print all accuracy scores in a single, comprehensive code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22ed29b2",
        "outputId": "ca3c94fb-ed0b-42db-ed9c-c5c98e48e929"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# --- Step 0: Load and Prepare Data ---\n",
        "wine = load_wine()\n",
        "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
        "y = pd.Series(wine.target, name='target')\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "print(\"Data preparation complete.\")\n",
        "\n",
        "# --- Step 1: Train KNN Model (Without Feature Scaling) ---\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = knn_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "print(f\"Accuracy of KNN without feature scaling: {accuracy_unscaled:.4f}\")\n",
        "\n",
        "# --- Step 2: Apply Feature Scaling ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "print(\"Feature scaling complete.\")\n",
        "\n",
        "# --- Step 3: Train KNN Model (With Feature Scaling) ---\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Accuracy of KNN with feature scaling: {accuracy_scaled:.4f}\")\n",
        "\n",
        "# --- Step 4: Apply PCA with 2 Components ---\n",
        "pca_2_components = PCA(n_components=2)\n",
        "pca_2_components.fit(X_train_scaled)\n",
        "X_train_pca = pca_2_components.transform(X_train_scaled)\n",
        "X_test_pca = pca_2_components.transform(X_test_scaled)\n",
        "print(\"PCA transformation complete with 2 components.\")\n",
        "\n",
        "# --- Step 5: Train KNN on PCA Data ---\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "print(f\"Accuracy of KNN with 2-component PCA: {accuracy_pca:.4f}\")\n",
        "\n",
        "# --- Step 6: Compare All Model Accuracies ---\n",
        "print(\"\\n--- Comparison of Accuracies ---\")\n",
        "print(f\"Accuracy of KNN without feature scaling: {accuracy_unscaled:.4f}\")\n",
        "print(f\"Accuracy of KNN with feature scaling: {accuracy_scaled:.4f}\")\n",
        "print(f\"Accuracy of KNN with 2-component PCA: {accuracy_pca:.4f}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preparation complete.\n",
            "Accuracy of KNN without feature scaling: 0.7407\n",
            "Feature scaling complete.\n",
            "Accuracy of KNN with feature scaling: 0.9630\n",
            "PCA transformation complete with 2 components.\n",
            "Accuracy of KNN with 2-component PCA: 0.9815\n",
            "\n",
            "--- Comparison of Accuracies ---\n",
            "Accuracy of KNN without feature scaling: 0.7407\n",
            "Accuracy of KNN with feature scaling: 0.9630\n",
            "Accuracy of KNN with 2-component PCA: 0.9815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "170328d7"
      },
      "source": [
        "### Comparison of KNN Accuracies:\n",
        "\n",
        "*   **KNN without Feature Scaling:** `accuracy_unscaled` = 0.7407\n",
        "*   **KNN with Feature Scaling:** `accuracy_scaled` = 0.9630\n",
        "*   **KNN with 2-component PCA:** `accuracy_pca` = 0.9815\n",
        "\n",
        "### Summary of Findings:\n",
        "\n",
        "The comparison clearly shows the significant impact of preprocessing steps on the performance of the K-Nearest Neighbors (KNN) classifier:\n",
        "\n",
        "1.  **Impact of Feature Scaling:** Applying `StandardScaler` dramatically improved the KNN model's accuracy from approximately **74.07% to 96.30%**. This highlights that KNN, being a distance-based algorithm, is highly sensitive to the scale of features. Without scaling, features with larger ranges dominate the distance calculations, leading to suboptimal neighbor identification.\n",
        "\n",
        "2.  **Impact of PCA:** Further applying PCA to reduce dimensionality to just 2 principal components, after feature scaling, resulted in an even higher accuracy of **98.15%**. This demonstrates that even with a substantial reduction in dimensionality (from 13 features to 2), PCA can preserve enough critical information to allow KNN to perform exceptionally well. In fact, reducing the number of features also helps mitigate the 'curse of dimensionality', making the 'nearest neighbor' concept more meaningful by focusing on the most variance-explaining components and potentially reducing noise.\n",
        "\n",
        "3.  **Synergy of PCA and Scaling:** The combination of feature scaling and PCA not only improved accuracy but also offers benefits in terms of computational efficiency (due to fewer dimensions) and potentially better generalization by focusing on the most relevant features while suppressing noise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2a4710f"
      },
      "source": [
        "### Comparison of KNN Accuracies:\n",
        "\n",
        "*   **KNN without Feature Scaling:** `accuracy_unscaled` = 0.7407\n",
        "*   **KNN with Feature Scaling:** `accuracy_scaled` = 0.9630\n",
        "*   **KNN with 2-component PCA:** `accuracy_pca` = 0.9815\n",
        "\n",
        "### Summary of Findings:\n",
        "\n",
        "The comparison clearly shows the significant impact of preprocessing steps on the performance of the K-Nearest Neighbors (KNN) classifier:\n",
        "\n",
        "1.  **Impact of Feature Scaling:** Applying `StandardScaler` dramatically improved the KNN model's accuracy from approximately **74.07% to 96.30%**. This highlights that KNN, being a distance-based algorithm, is highly sensitive to the scale of features. Without scaling, features with larger ranges dominate the distance calculations, leading to suboptimal neighbor identification.\n",
        "\n",
        "2.  **Impact of PCA:** Further applying PCA to reduce dimensionality to just 2 principal components, after feature scaling, resulted in an even higher accuracy of **98.15%**. This demonstrates that even with a substantial reduction in dimensionality (from 13 features to 2), PCA can preserve enough critical information to allow KNN to perform exceptionally well. In fact, reducing the number of features also helps mitigate the 'curse of dimensionality', making the 'nearest neighbor' concept more meaningful by focusing on the most variance-explaining components and potentially reducing noise.\n",
        "\n",
        "3.  **Synergy of PCA and Scaling:** The combination of feature scaling and PCA not only improved accuracy but also offers benefits in terms of computational efficiency (due to fewer dimensions) and potentially better generalization by focusing on the most relevant features while suppressing noise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8ea2fa3"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the findings regarding the impact of PCA on KNN accuracy and how it compares to using scaled and unscaled original features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55a0fad2"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The impact of PCA on KNN accuracy, when applied after feature scaling, is a marginal but positive improvement. KNN accuracy using 2-component PCA on scaled data was 98.15\\%. This is slightly higher than KNN accuracy using only scaled original features, which was 96.30\\%. Both of these significantly outperform KNN on unscaled original features, which had an accuracy of 74.07\\%. This demonstrates that PCA can preserve critical information even with substantial dimensionality reduction, leading to comparable or slightly better performance than using all scaled features for KNN.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   K-Nearest Neighbors (KNN) classifier achieved an accuracy of 74.07% when trained on unscaled data.\n",
        "*   Applying feature scaling with `StandardScaler` dramatically improved KNN accuracy to 96.30%.\n",
        "*   Further applying PCA to reduce dimensionality to 2 principal components, after feature scaling, resulted in the highest KNN accuracy of 98.15\\%.\n",
        "*   PCA successfully reduced the data's dimensionality from 13 features to 2 components, while retaining and slightly enhancing the predictive performance of the KNN model.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   Feature scaling is a critical preprocessing step for distance-based algorithms like KNN, significantly enhancing model performance by preventing features with larger ranges from dominating distance calculations.\n",
        "*   PCA, when applied after appropriate scaling, can effectively reduce dimensionality while maintaining or even improving model accuracy, offering benefits in terms of computational efficiency and potentially mitigating the curse of dimensionality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.**\n",
        "\n",
        "\n",
        "K-Nearest Neighbors (KNN) classifiers using different distance metrics (Euclidean and Manhattan) on the scaled Wine dataset and compare their performance. Here's the plan:\n",
        "\n",
        "Train KNN with Euclidean Distance: Train a KNeighborsClassifier on the scaled training data using 'euclidean' distance (default) and evaluate its accuracy on the scaled test data. Store the accuracy.\n",
        "\n",
        "Train KNN with Manhattan Distance: Train a KNeighborsClassifier on the scaled training data using 'manhattan' distance and evaluate its accuracy on the scaled test data. Store the accuracy.\n",
        "\n",
        "Compare Distance Metric Results: Print and compare the accuracy scores obtained from KNN models using Euclidean and Manhattan distance metrics. Discuss which metric performed better for the Wine dataset.\n",
        "\n",
        "Final Task: Summarize the findings regarding the impact of different distance metrics on KNN accuracy for the scaled Wine dataset."
      ],
      "metadata": {
        "id": "HHh6JmUdlVzF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "721d8aed"
      },
      "source": [
        "# Task\n",
        "**Train KNN with Euclidean Distance**: Train a KNeighborsClassifier on the scaled training data (`X_train_scaled`, `y_train`) using 'euclidean' distance (default) and evaluate its accuracy on the scaled test data (`X_test_scaled`, `y_test`). Store the accuracy in a variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a9f5fd7"
      },
      "source": [
        "## Train KNN with Euclidean Distance\n",
        "\n",
        "### Subtask:\n",
        "Train a KNeighborsClassifier on the scaled training data (`X_train_scaled`, `y_train`) using 'euclidean' distance (default) and evaluate its accuracy on the scaled test data (`X_test_scaled`, `y_test`). Store the accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49e78a18"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires training a KNN classifier with Euclidean distance on scaled data and evaluating its accuracy. I will instantiate the KNeighborsClassifier with the specified parameters, fit it, make predictions, and calculate the accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "901760e6",
        "outputId": "e2a3225a-7cea-4c66-9a39-94fd43307a34"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Instantiate a KNeighborsClassifier with n_neighbors=5 and metric='euclidean'\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "\n",
        "# 2. Fit the KNN model to the scaled training data\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 3. Make predictions on the scaled test data\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "\n",
        "# 4. Calculate the accuracy score\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "print(f\"Accuracy of KNN with Euclidean distance: {accuracy_euclidean:.4f}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of KNN with Euclidean distance: 0.9630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d311baa"
      },
      "source": [
        "## Train KNN with Manhattan Distance\n",
        "\n",
        "### Subtask:\n",
        "Train a KNeighborsClassifier on the scaled training data (`X_train_scaled`, `y_train`) using 'manhattan' distance and evaluate its accuracy on the scaled test data (`X_test_scaled`, `y_test`). Store the accuracy.\n",
        "\n",
        "#### Instructions:\n",
        "1. Instantiate a `KNeighborsClassifier` with `n_neighbors=5` and `metric='manhattan'`.\n",
        "2. Fit the KNN model to the scaled training data (`X_train_scaled`, `y_train`).\n",
        "3. Make predictions on the scaled test data (`X_test_scaled`).\n",
        "4. Calculate the accuracy score of the model using `accuracy_score` and store it in a variable named `accuracy_manhattan`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "796e2ca3"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires training a KNN classifier with Manhattan distance on scaled data and evaluating its accuracy. I will instantiate the KNeighborsClassifier with the specified parameters, fit it, make predictions, and calculate the accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0e0d2c0",
        "outputId": "7db07a23-5d14-4903-b156-0c55e93fd2ce"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Instantiate a KNeighborsClassifier with n_neighbors=5 and metric='manhattan'\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "\n",
        "# 2. Fit the KNN model to the scaled training data\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 3. Make predictions on the scaled test data\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "\n",
        "# 4. Calculate the accuracy score\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "print(f\"Accuracy of KNN with Manhattan distance: {accuracy_manhattan:.4f}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of KNN with Manhattan distance: 0.9630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "366bbd31"
      },
      "source": [
        "## Compare KNN Accuracy with Different Distance Metrics\n",
        "\n",
        "### Subtask:\n",
        "Compare the accuracy scores of the KNN models trained with Euclidean and Manhattan distance metrics on the scaled dataset. Summarize the findings to understand the impact of the distance metric on KNN performance.\n",
        "\n",
        "#### Instructions:\n",
        "1. Print both `accuracy_euclidean` and `accuracy_manhattan` for a direct comparison.\n",
        "2. Provide a brief explanation of the observed differences or similarities in performance between the two distance metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85258463"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous steps have calculated the accuracy for KNN using both Euclidean and Manhattan distances on scaled data. Now, as per the subtask, I need to print these accuracies for a direct comparison and then provide an explanation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3253d293",
        "outputId": "be2902db-802a-4b70-fae0-daf9f9bdb53b"
      },
      "source": [
        "print(f\"Accuracy of KNN with Euclidean distance: {accuracy_euclidean:.4f}\")\n",
        "print(f\"Accuracy of KNN with Manhattan distance: {accuracy_manhattan:.4f}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of KNN with Euclidean distance: 0.9630\n",
            "Accuracy of KNN with Manhattan distance: 0.9630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "404ffeb4"
      },
      "source": [
        "### Comparison of KNN Accuracies with Different Distance Metrics:\n",
        "\n",
        "*   **KNN with Euclidean Distance:** `accuracy_euclidean` = 0.9630\n",
        "*   **KNN with Manhattan Distance:** `accuracy_manhattan` = 0.9630\n",
        "\n",
        "### Summary of Findings:\n",
        "\n",
        "In this particular case, for the Wine dataset after feature scaling, both Euclidean and Manhattan distance metrics yielded the **same accuracy of 96.30%** for the K-Nearest Neighbors (KNN) classifier with `n_neighbors=5`.\n",
        "\n",
        "This outcome suggests that for this specific dataset and choice of `k`, the way distances are measured (straight line vs. sum of absolute differences) did not significantly alter the identification of nearest neighbors or the classification outcome. It's possible that the data's distribution in the scaled feature space is such that the relative proximity of points is maintained similarly by both metrics, or that the decision boundaries are clear enough that slight variations in distance calculation do not affect the majority class prediction for the given `k`.\n",
        "\n",
        "While they produced identical results here, in other datasets or with different `k` values, the choice of distance metric can indeed have an impact. Euclidean distance (L2 norm) is sensitive to outliers and works well in dense spaces, while Manhattan distance (L1 norm) is more robust to outliers and is often preferred in high-dimensional spaces or when dealing with features that have different units. For this Wine dataset, after `StandardScaler` normalized the features, both metrics performed equally well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77261029"
      },
      "source": [
        "### Comparison of KNN Accuracies with Different Distance Metrics:\n",
        "\n",
        "*   **KNN with Euclidean Distance:** `accuracy_euclidean` = 0.9630\n",
        "*   **KNN with Manhattan Distance:** `accuracy_manhattan` = 0.9630\n",
        "\n",
        "### Summary of Findings:\n",
        "\n",
        "In this particular case, for the Wine dataset after feature scaling, both Euclidean and Manhattan distance metrics yielded the **same accuracy of 96.30%** for the K-Nearest Neighbors (KNN) classifier with `n_neighbors=5`.\n",
        "\n",
        "This outcome suggests that for this specific dataset and choice of `k`, the way distances are measured (straight line vs. sum of absolute differences) did not significantly alter the identification of nearest neighbors or the classification outcome. It's possible that the data's distribution in the scaled feature space is such that the relative proximity of points is maintained similarly by both metrics, or that the decision boundaries are clear enough that slight variations in distance calculation do not affect the majority class prediction for the given `k`.\n",
        "\n",
        "While they produced identical results here, in other datasets or with different `k` values, the choice of distance metric can indeed have an impact. Euclidean distance (L2 norm) is sensitive to outliers and works well in dense spaces, while Manhattan distance (L1 norm) is more robust to outliers and is often preferred in high-dimensional spaces or when dealing with features that have different units. For this Wine dataset, after `StandardScaler` normalized the features, both metrics performed equally well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "904211c2"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the findings regarding the impact of different distance metrics on KNN accuracy for the scaled Wine dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4aa25e2"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The impact of different distance metrics (Euclidean and Manhattan) on KNN accuracy for the scaled Wine dataset was evaluated. It was found that both metrics yielded identical accuracy scores of 96.30% for the KNN classifier with n\\_neighbors=5.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The K-Nearest Neighbors (KNN) classifier, when trained with Euclidean distance on the scaled Wine dataset, achieved an accuracy of 96.30%.\n",
        "*   The KNN classifier, when trained with Manhattan distance on the scaled Wine dataset, also achieved an identical accuracy of 96.30%.\n",
        "*   For the given Wine dataset, after feature scaling, and with n\\_neighbors set to 5, both Euclidean and Manhattan distance metrics resulted in precisely the same classification performance.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   For this particular scaled Wine dataset and chosen `k` value, the relative proximity of data points was consistent across both Euclidean and Manhattan distance calculations, leading to no discernible difference in classification accuracy.\n",
        "*   While they performed equally here, in other datasets or with different `k` values, the choice of distance metric can significantly impact KNN performance. Further analysis could explore the effect of varying `k` values or applying these models to different datasets to observe scenarios where distance metrics yield varied results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer.\n",
        "\n",
        "Due to the large number of features and a small number of samples, traditional models overfit.\n",
        "\n",
        "Explain how you would:\n",
        "\n",
        " Use PCA to reduce dimensionality    \n",
        " Decide how many components to keep    \n",
        " Use KNN for classification post-dimensionality reduction    \n",
        " Evaluate the model    \n",
        " Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data    \n",
        "\n",
        "\n",
        "1. Use PCA to reduce dimensionality\n",
        "Given a high-dimensional gene expression dataset, PCA would be applied as a crucial preprocessing step. The steps would involve:\n",
        "\n",
        "Data Preprocessing: First, the raw gene expression data would need to be thoroughly preprocessed. This includes quality control, normalization (e.g., to account for differences in sequencing depth or microarray intensity), and often a log-transformation to stabilize variance and make the data more normally distributed. Missing values would also need to be imputed or handled appropriately.\n",
        "Feature Scaling: Before applying PCA, it's essential to standardize the features (genes). This ensures that genes with naturally higher expression levels don't disproportionately influence the principal components. StandardScaler (subtracting the mean and dividing by the standard deviation) would be applied to each gene across all samples.\n",
        "Applying PCA: The PCA algorithm would then be fitted to the scaled training data. PCA identifies new orthogonal axes (principal components) that capture the maximum variance in the data. These principal components are linear combinations of the original gene features.\n",
        "Transformation: Both the scaled training data and future unseen test data would be transformed into this new, lower-dimensional space defined by the selected principal components. Each sample would then be represented by a set of principal component scores instead of its original thousands of gene expression values.\n",
        "2. Decide how many components to keep\n",
        "Deciding the optimal number of principal components (PCs) to retain is critical to balancing dimensionality reduction with information preservation. I would use a combination of methods:\n",
        "\n",
        "Explained Variance Ratio: After fitting PCA, I would examine the explained_variance_ratio_ attribute, which tells us the proportion of variance each PC explains. I would plot a scree plot (individual explained variance per component) and a cumulative explained variance plot.\n",
        "Threshold-based Selection: A common approach is to select PCs that explain a cumulative percentage of the total variance, such as 90% or 95%. This threshold ensures that most of the original data's variability is preserved.\n",
        "Kaiser's Rule (Eigenvalues > 1): This rule suggests keeping only principal components with an eigenvalue greater than 1. An eigenvalue > 1 indicates that the PC explains more variance than a single original standardized variable.\n",
        "Domain Knowledge/Interpretability: If certain components have a clear biological interpretation (e.g., correlating with known biological pathways or processes), they might be prioritized. However, for a fully automated pipeline, threshold-based methods are usually more objective.\n",
        "Cross-validation with Downstream Task: Ultimately, the best number of components for classification is the one that yields the best performance for the KNN classifier. I would integrate the PCA dimensionality reduction within a cross-validation loop and test different numbers of components to see which one optimizes the KNN's classification accuracy on validation sets.\n",
        "3. Use KNN for classification post-dimensionality reduction\n",
        "Once the dimensionality is reduced, the KNN classifier can be effectively applied:\n",
        "\n",
        "Training: The KNN classifier (KNeighborsClassifier) would be trained on the X_train_pca (PCA-transformed training data) and y_train (corresponding cancer types). The n_neighbors hyperparameter would be tuned (e.g., via cross-validation, typically an odd number like 3, 5, or 7).\n",
        "Prediction: For a new patient sample, it would first undergo the same preprocessing (normalization, scaling) and then be transformed using the same fitted PCA model to get its principal component scores (X_new_pca). This X_new_pca would then be fed to the trained KNN model to predict the cancer type.\n",
        "4. Evaluate the model\n",
        "Model evaluation is crucial for assessing performance and ensuring generalizability. Given the small sample size, robust evaluation techniques are particularly important:\n",
        "\n",
        "Cross-Validation: Due to the small number of samples, a strong emphasis would be placed on k-fold cross-validation (e.g., 5-fold or 10-fold CV) or Leave-One-Out Cross-Validation (LOOCV) to get a reliable estimate of the model's performance and reduce bias from a single train-test split.\n",
        "Metrics: For classification, I would evaluate standard metrics:\n",
        "Accuracy: Overall correct predictions.\n",
        "Precision, Recall, F1-score: Especially important for imbalanced datasets (e.g., if one cancer type is much rarer).\n",
        "ROC AUC (Receiver Operating Characteristic Area Under the Curve): A good measure of the model's ability to distinguish between classes, particularly useful in multi-class scenarios (macro or micro averaging).\n",
        "Confusion Matrix: To visualize specific misclassifications (e.g., which cancer types are confused with others).\n",
        "Performance on Test Set: The final model would be evaluated on a completely unseen test set (held out from the very beginning) to provide an unbiased estimate of its real-world performance.\n",
        "5. Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data\n",
        "I would present the following justifications to stakeholders:\n",
        "\n",
        "Addresses High Dimensionality and Small N: \"Our gene expression data is extremely high-dimensional, a classic challenge where traditional models often fail by overfitting. With a limited number of patient samples, directly applying models leads to poor generalization. This pipeline tackles this head-on.\"\n",
        "Mitigates Overfitting (PCA): \"PCA is a powerful technique that extracts the most essential information (variance) from thousands of genes into a few principal components. By focusing on these core patterns, we effectively filter out noise and reduce the risk of overfitting, making our model more stable and reliable for new patient data.\"\n",
        "Reduces 'Curse of Dimensionality': \"The 'Curse of Dimensionality' makes it hard for distance-based algorithms like KNN to find meaningful relationships in high-dimensional space. PCA transforms the data into a lower-dimensional representation where the concept of 'nearest' truly reflects biological similarity, thereby improving KNN's effectiveness.\"\n",
        "Computational Efficiency: \"Working with thousands of genes is computationally intensive. PCA significantly reduces the data size, making our classification model faster to train and more efficient for real-time predictions in a clinical setting.\"\n",
        "Robustness of KNN: \"KNN is a non-parametric and intuitive algorithm. It makes no assumptions about the underlying data distribution, which is advantageous for complex biological data. It classifies new patients based on the cancer types of their most similar genetic profiles found in our existing data.\"\n",
        "Interpretable Foundation: \"While PCA components are abstract, we can analyze the loadings to understand which original genes contribute most to the differentiating principal components, offering avenues for biological insights and further research.\"\n",
        "Rigorous Evaluation: \"We employ robust cross-validation techniques and evaluate the model using multiple clinically relevant metrics (accuracy, precision, recall, AUC) on an independent test set. This ensures our reported performance is a trustworthy estimate of how well the model will perform on new, unseen patients, a critical factor for biomedical applications.\"\n",
        "Proven Methodology: \"This PCA-KNN pipeline is a well-established and scientifically accepted methodology for handling high-dimensional data, particularly in fields like bioinformatics, where feature reduction is often a prerequisite for effective machine learning.\""
      ],
      "metadata": {
        "id": "ZXM8LlIXl21P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement the PCA + KNN pipeline for a high-dimensional gene expression dataset, I will adapt the process using the Wine dataset, as no specific gene expression dataset has been provided. This will demonstrate the pipeline described in Question 10. Here's the plan:\n",
        "\n",
        "Load, Split, and Scale Data: Load the Wine dataset, split it into training and testing sets, and apply StandardScaler to scale the features. This sets up the data in a high-dimensional (relative to the problem) and scaled format, ready for PCA.    \n",
        "\n",
        "Apply PCA with Optimal Components: Apply PCA to the scaled training data. Determine the number of principal components to retain based on explaining a high cumulative percentage of variance (e.g., 95%). Transform both the scaled training and testing data using the selected components.   \n",
        "\n",
        "Train KNN on PCA-Transformed Data: Train a K-Nearest Neighbors (KNN) classifier on the PCA-transformed training data. For consistency, use n_neighbors=5.   \n",
        "\n",
        "Evaluate PCA-KNN Model: Make predictions using the trained KNN model on the PCA-transformed test data and calculate its accuracy. Compare this accuracy to the previously obtained results (KNN with scaled original features).\n",
        "  \n",
        "Final Task: Summarize the implementation of the PCA+KNN pipeline and discuss its performance in the context of high-dimensional data, linking back to the justifications provided in Question 10."
      ],
      "metadata": {
        "id": "dsVLEfdMmqUo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b3cf040"
      },
      "source": [
        "# Task\n",
        "The task is to apply PCA with an optimal number of components to the scaled Wine dataset, ensuring that a high cumulative percentage of variance (e.g., 95%) is explained. Then, train a KNN classifier on this PCA-transformed data, evaluate its accuracy, and compare it with previous results to summarize the findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44d088e7"
      },
      "source": [
        "## Load, Split, and Scale Data\n",
        "\n",
        "### Subtask:\n",
        "Load the Wine dataset, split it into training and testing sets, and apply StandardScaler to scale the features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "144db03a"
      },
      "source": [
        "**Reasoning**:\n",
        "I will load the Wine dataset, extract features and target, split the data into training and testing sets, and then apply StandardScaler to scale the features, as instructed in the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89a4ada7",
        "outputId": "d65f1cdb-2450-4985-aeda-087644e4a857"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine = load_wine()\n",
        "\n",
        "# 2. Extract features (X) and target (y)\n",
        "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
        "y = pd.Series(wine.target, name='target')\n",
        "\n",
        "# 3. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 4. Instantiate StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# 5. Fit the scaler on X_train and transform both X_train and X_test\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Data loaded, split, and scaled successfully.\")\n",
        "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
        "print(f\"X_test_scaled shape: {X_test_scaled.shape}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded, split, and scaled successfully.\n",
            "X_train_scaled shape: (124, 13)\n",
            "X_test_scaled shape: (54, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1bd24ce"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the data is scaled, I will apply PCA to the scaled training data to determine the explained variance ratio for each principal component and find the number of components required to explain at least 95% of the variance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e846c86e",
        "outputId": "a73c2b17-5c87-4ab6-e763-ab919e20b7cf"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# Instantiate PCA to find all components\n",
        "pca = PCA(n_components=None)\n",
        "\n",
        "# Fit PCA on the scaled training data\n",
        "pca.fit(X_train_scaled)\n",
        "\n",
        "# Calculate cumulative explained variance\n",
        "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
        "\n",
        "# Find the number of components that explain at least 95% variance\n",
        "n_components_95 = np.where(cumulative_variance >= 0.95)[0][0] + 1\n",
        "\n",
        "print(\"Explained variance ratio of each principal component:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"Principal Component {i+1}: {ratio:.4f}\")\n",
        "\n",
        "print(\"\\nCumulative explained variance:\")\n",
        "for i, cum_ratio in enumerate(cumulative_variance):\n",
        "    print(f\"Up to Principal Component {i+1}: {cum_ratio:.4f}\")\n",
        "\n",
        "print(f\"\\nNumber of components to explain >= 95% variance: {n_components_95}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio of each principal component:\n",
            "Principal Component 1: 0.3620\n",
            "Principal Component 2: 0.1876\n",
            "Principal Component 3: 0.1166\n",
            "Principal Component 4: 0.0758\n",
            "Principal Component 5: 0.0704\n",
            "Principal Component 6: 0.0455\n",
            "Principal Component 7: 0.0358\n",
            "Principal Component 8: 0.0265\n",
            "Principal Component 9: 0.0217\n",
            "Principal Component 10: 0.0196\n",
            "Principal Component 11: 0.0176\n",
            "Principal Component 12: 0.0132\n",
            "Principal Component 13: 0.0076\n",
            "\n",
            "Cumulative explained variance:\n",
            "Up to Principal Component 1: 0.3620\n",
            "Up to Principal Component 2: 0.5496\n",
            "Up to Principal Component 3: 0.6662\n",
            "Up to Principal Component 4: 0.7420\n",
            "Up to Principal Component 5: 0.8124\n",
            "Up to Principal Component 6: 0.8579\n",
            "Up to Principal Component 7: 0.8938\n",
            "Up to Principal Component 8: 0.9202\n",
            "Up to Principal Component 9: 0.9420\n",
            "Up to Principal Component 10: 0.9616\n",
            "Up to Principal Component 11: 0.9792\n",
            "Up to Principal Component 12: 0.9924\n",
            "Up to Principal Component 13: 1.0000\n",
            "\n",
            "Number of components to explain >= 95% variance: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7ad52db"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the optimal number of principal components (10) has been identified to explain 95% of the variance, I will apply PCA with this number of components to transform the scaled training and testing data. This prepares the data for the next step: training the KNN classifier on the dimensionality-reduced data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc1618ea",
        "outputId": "1bf00329-b6a8-4205-db44-6d5246d4a096"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Instantiate PCA with the determined number of components (n_components_95)\n",
        "pca_optimal = PCA(n_components=n_components_95)\n",
        "\n",
        "# Fit PCA on the scaled training data and transform both training and testing data\n",
        "X_train_pca_optimal = pca_optimal.fit_transform(X_train_scaled)\n",
        "X_test_pca_optimal = pca_optimal.transform(X_test_scaled)\n",
        "\n",
        "print(f\"PCA transformation complete with {n_components_95} components:\")\n",
        "print(f\"X_train_pca_optimal shape: {X_train_pca_optimal.shape}\")\n",
        "print(f\"X_test_pca_optimal shape: {X_test_pca_optimal.shape}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCA transformation complete with 10 components:\n",
            "X_train_pca_optimal shape: (124, 10)\n",
            "X_test_pca_optimal shape: (54, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ffb5d1b"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the data has been transformed using PCA with the optimal number of components, I will train a K-Nearest Neighbors (KNN) classifier on this reduced-dimensional training data, as specified in the overall task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db304a6a",
        "outputId": "268f7f00-50e7-45d4-8a50-02ec07da45de"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Instantiate a KNeighborsClassifier with n_neighbors=5 (consistent with previous tasks)\n",
        "knn_pca_optimal = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Fit the KNN model to the PCA-transformed training data\n",
        "knn_pca_optimal.fit(X_train_pca_optimal, y_train)\n",
        "\n",
        "print(\"KNN model trained on optimally PCA-transformed data.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN model trained on optimally PCA-transformed data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14cce005"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the KNN model has been trained on the optimally PCA-transformed data, the next step is to evaluate its performance by making predictions on the corresponding test data and calculating the accuracy score, as per the overall task instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "593f4946",
        "outputId": "aaa0b1c4-a9f4-4164-9631-e3be4b1c4825"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Make predictions on the optimally PCA-transformed test data\n",
        "y_pred_pca_optimal = knn_pca_optimal.predict(X_test_pca_optimal)\n",
        "\n",
        "# Calculate the accuracy score\n",
        "accuracy_pca_optimal = accuracy_score(y_test, y_pred_pca_optimal)\n",
        "\n",
        "print(f\"Accuracy of KNN with optimally selected PCA components ({n_components_95} components): {accuracy_pca_optimal:.4f}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of KNN with optimally selected PCA components (10 components): 0.9630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4664e46f"
      },
      "source": [
        "## Summary of PCA + KNN Pipeline Implementation\n",
        "\n",
        "### Implementation Steps and Findings:\n",
        "\n",
        "1.  **Data Loading, Splitting, and Scaling:**\n",
        "    *   The Wine dataset was loaded, split into training (124 samples) and testing (54 samples) sets.\n",
        "    *   `StandardScaler` was applied to the features, which is crucial for PCA as it's sensitive to feature scales and for KNN as it's a distance-based algorithm.\n",
        "\n",
        "2.  **PCA Application and Optimal Component Selection:**\n",
        "    *   PCA was applied to the scaled training data.\n",
        "    *   Analysis of the explained variance ratio showed that **10 principal components** were sufficient to capture approximately **96.16%** of the total variance in the dataset. This number was chosen as the 'optimal' number of components.\n",
        "    *   Both the scaled training and testing data were transformed into this 10-dimensional PCA space (`X_train_pca_optimal`, `X_test_pca_optimal`).\n",
        "\n",
        "3.  **KNN Training on PCA-Transformed Data:**\n",
        "    *   A `KNeighborsClassifier` with `n_neighbors=5` was trained on the `X_train_pca_optimal` data.\n",
        "\n",
        "4.  **Model Evaluation:**\n",
        "    *   The trained KNN model's accuracy was evaluated on the `X_test_pca_optimal` data.\n",
        "    *   **Accuracy of KNN with optimally selected PCA components (10 components): 0.9630**\n",
        "\n",
        "### Comparison with Previous Results:\n",
        "\n",
        "*   **KNN without Feature Scaling:** 0.7407\n",
        "*   **KNN with Feature Scaling:** 0.9630\n",
        "*   **KNN with 2-component PCA (previous task):** 0.9815\n",
        "*   **KNN with optimally selected PCA components (10 components):** 0.9630\n",
        "\n",
        "### Discussion and Justification for Stakeholders (linking to Question 10):\n",
        "\n",
        "The implementation of the PCA + KNN pipeline for the (simulated) high-dimensional gene expression data (using the Wine dataset as a proxy) demonstrates a robust approach, especially relevant for biomedical data with many features and potentially limited samples.\n",
        "\n",
        "1.  **Addressing High Dimensionality and Overfitting:** Our original dataset had 13 features. By reducing it to **10 principal components** while retaining over 95% of the variance, we effectively tackled the high dimensionality. This process intrinsically filters out noise and extracts the most significant underlying patterns, making the model less prone to overfitting, a critical concern with small sample sizes in gene expression data.\n",
        "\n",
        "2.  **Mitigating the Curse of Dimensionality:** KNN's performance can degrade in high-dimensional spaces. PCA transformed our data into a lower-dimensional space where the concept of 'nearest neighbors' is more meaningful and reliable, allowing KNN to identify true similarities more accurately.\n",
        "\n",
        "3.  **Computational Efficiency:** Operating on 10 components instead of 13 original features leads to faster distance calculations for KNN, making the model more efficient for both training and prediction, which is beneficial in clinical contexts where rapid analysis might be required.\n",
        "\n",
        "4.  **Performance and Robustness:** The accuracy of **0.9630** with 10 PCA components is identical to using all 13 scaled features. This demonstrates that we achieved significant dimensionality reduction without sacrificing predictive power. While the 2-component PCA achieved a slightly higher accuracy in the previous task, that was an isolated case; selecting components based on a variance threshold (like 95%) is a more systematic and robust approach for retaining information.\n",
        "\n",
        "5.  **Robust Evaluation:** The entire pipeline is built on robust techniques: `StandardScaler` for proper feature weighting, PCA for scientifically sound dimensionality reduction, and KNN for its non-parametric nature. Further rigorous cross-validation (as discussed in Q10) would solidify this performance on real biomedical data.\n",
        "\n",
        "This pipeline provides a balanced solution, ensuring that we extract the maximum relevant information from complex data while maintaining model simplicity, interpretability (through component analysis), and high predictive accuracy, making it a reliable choice for classifying cancer types from gene expression data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0a6e04c"
      },
      "source": [
        "## Summary of PCA + KNN Pipeline Implementation\n",
        "\n",
        "### Implementation Steps and Findings:\n",
        "\n",
        "1.  **Data Loading, Splitting, and Scaling:**\n",
        "    *   The Wine dataset was loaded, split into training (124 samples) and testing (54 samples) sets.\n",
        "    *   `StandardScaler` was applied to the features, which is crucial for PCA as it's sensitive to feature scales and for KNN as it's a distance-based algorithm.\n",
        "\n",
        "2.  **PCA Application and Optimal Component Selection:**\n",
        "    *   PCA was applied to the scaled training data.\n",
        "    *   Analysis of the explained variance ratio showed that **10 principal components** were sufficient to capture approximately **96.16%** of the total variance in the dataset. This number was chosen as the 'optimal' number of components.\n",
        "    *   Both the scaled training and testing data were transformed into this 10-dimensional PCA space (`X_train_pca_optimal`, `X_test_pca_optimal`).\n",
        "\n",
        "3.  **KNN Training on PCA-Transformed Data:**\n",
        "    *   A `KNeighborsClassifier` with `n_neighbors=5` was trained on the `X_train_pca_optimal` data.\n",
        "\n",
        "4.  **Model Evaluation:**\n",
        "    *   The trained KNN model's accuracy was evaluated on the `X_test_pca_optimal` data.\n",
        "    *   **Accuracy of KNN with optimally selected PCA components (10 components): 0.9630**\n",
        "\n",
        "### Comparison with Previous Results:\n",
        "\n",
        "*   **KNN without Feature Scaling:** 0.7407\n",
        "*   **KNN with Feature Scaling:** 0.9630\n",
        "*   **KNN with 2-component PCA (previous task):** 0.9815\n",
        "*   **KNN with optimally selected PCA components (10 components):** 0.9630\n",
        "\n",
        "### Discussion and Justification for Stakeholders (linking to Question 10):\n",
        "\n",
        "The implementation of the PCA + KNN pipeline for the (simulated) high-dimensional gene expression data (using the Wine dataset as a proxy) demonstrates a robust approach, especially relevant for biomedical data with many features and potentially limited samples.\n",
        "\n",
        "1.  **Addressing High Dimensionality and Overfitting:** Our original dataset had 13 features. By reducing it to **10 principal components** while retaining over 95% of the variance, we effectively tackled the high dimensionality. This process intrinsically filters out noise and extracts the most significant underlying patterns, making the model less prone to overfitting, a critical concern with small sample sizes in gene expression data.\n",
        "\n",
        "2.  **Mitigating the Curse of Dimensionality:** KNN's performance can degrade in high-dimensional spaces. PCA transformed our data into a lower-dimensional space where the concept of 'nearest neighbors' is more meaningful and reliable, allowing KNN to identify true similarities more accurately.\n",
        "\n",
        "3.  **Computational Efficiency:** Operating on 10 components instead of 13 original features leads to faster distance calculations for KNN, making the model more efficient for both training and prediction, which is beneficial in clinical contexts where rapid analysis might be required.\n",
        "\n",
        "4.  **Performance and Robustness:** The accuracy of **0.9630** with 10 PCA components is identical to using all 13 scaled features. This demonstrates that we achieved significant dimensionality reduction without sacrificing predictive power. While the 2-component PCA achieved a slightly higher accuracy in the previous task, that was an isolated case; selecting components based on a variance threshold (like 95%) is a more systematic and robust approach for retaining information.\n",
        "\n",
        "5.  **Robust Evaluation:** The entire pipeline is built on robust techniques: `StandardScaler` for proper feature weighting, PCA for scientifically sound dimensionality reduction, and KNN for its non-parametric nature. Further rigorous cross-validation (as discussed in Q10) would solidify this performance on real biomedical data.\n",
        "\n",
        "This pipeline provides a balanced solution, ensuring that we extract the maximum relevant information from complex data while maintaining model simplicity, interpretability (through component analysis), and high predictive accuracy, making it a reliable choice for classifying cancer types from gene expression data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5fc9ba5"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the implementation of the PCA+KNN pipeline and discuss its performance in the context of high-dimensional data, linking back to the justifications provided in Question 10.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28b71ffe"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The implementation of the PCA+KNN pipeline involved several steps:\n",
        "1.  **Data Preparation**: The Wine dataset was loaded, split into training (124 samples) and testing (54 samples) sets, and features were scaled using `StandardScaler`. This scaling is crucial for both PCA (due to its sensitivity to feature scales) and KNN (as a distance-based algorithm).\n",
        "2.  **PCA Application and Optimal Component Selection**: PCA was applied to the scaled training data. Analysis revealed that **10 principal components** were sufficient to capture approximately **96.16%** of the total variance, meeting the objective of retaining at least 95% variance. Both training and testing data were then transformed into this 10-dimensional PCA space.\n",
        "3.  **KNN Training**: A `KNeighborsClassifier` with `n_neighbors=5` was trained on the PCA-transformed training data.\n",
        "4.  **Model Evaluation**: The trained KNN model achieved an accuracy of **0.9630** on the PCA-transformed test data.\n",
        "\n",
        "In the context of high-dimensional data (simulated by the Wine dataset as a proxy for gene expression data), the PCA+KNN pipeline performs robustly due to the following justifications:\n",
        "*   **Addressing High Dimensionality and Overfitting**: By reducing the original 13 features to 10 principal components while retaining over 95% of the variance, the pipeline effectively tackles high dimensionality. This process filters noise and extracts significant patterns, making the model less prone to overfitting, which is critical for datasets with many features and limited samples.\n",
        "*   **Mitigating the Curse of Dimensionality**: PCA transforms data into a lower-dimensional space, where the concept of 'nearest neighbors' becomes more meaningful, enhancing KNN's ability to identify true similarities.\n",
        "*   **Computational Efficiency**: Operating on 10 components instead of 13 original features leads to faster distance calculations for KNN, improving both training and prediction efficiency.\n",
        "*   **Performance and Robustness**: The achieved accuracy of 0.9630 with 10 PCA components is identical to using all 13 scaled features. This demonstrates that significant dimensionality reduction was achieved without sacrificing predictive power. Selecting components based on a variance threshold (e.g., 95%) offers a more systematic and robust approach compared to arbitrary component selection.\n",
        "*   **Robust Evaluation**: The pipeline leverages robust techniques: `StandardScaler` for proper feature weighting, PCA for scientifically sound dimensionality reduction, and KNN for its non-parametric nature.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The Wine dataset was successfully loaded, split into training (124 samples) and testing (54 samples) sets, and its 13 features were scaled using `StandardScaler`.\n",
        "*   PCA analysis determined that **10 principal components** were required to explain at least 95% of the cumulative variance, specifically explaining approximately **96.16%**.\n",
        "*   The dataset was successfully transformed, reducing its dimensionality from 13 features to 10 principal components.\n",
        "*   A K-Nearest Neighbors (KNN) classifier, trained on the 10-component PCA-transformed data, achieved an accuracy of **0.9630** on the test set.\n",
        "*   This accuracy is identical to the performance of KNN when only feature scaling was applied (0.9630) but slightly lower than a previous result using only 2 PCA components (0.9815).\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The PCA+KNN pipeline effectively reduces dataset dimensionality by 23% (from 13 to 10 features) while maintaining identical predictive accuracy to a model trained on all scaled features, highlighting its efficiency and robustness in handling high-dimensional data.\n",
        "*   To further validate the robustness and generalization capability of this pipeline, especially for real-world biomedical data, implementing rigorous cross-validation techniques would be a crucial next step.\n"
      ]
    }
  ]
}